<!--#include virtual="/inc/header02.txt" -->
<title>GIS-Lab: Матрица ошибок и расчет показателей точности тематических карт</title>
<!--#include virtual="/inc/header2.txt" -->
<div class="cont">
<div class="col1">

<ul class="path">
   <li class="first"><a href="/">Главная</a></li>
   <li><a href="/qa.html">Вопросы и ответы</a></li>
</ul>
<!--Contents start-->
<h1>Матрица ошибок и расчет показателей точности тематических карт</h1>
<p class="ann">Дано  определение матрицы ошибок (confusion matrix, contingency table, error matrix), приведены примеры использования.</p>
<p class="discuss discuss_top"><span><!--#include virtual="/scripts/forum-comments-num.php?i=4601"--></span></p>


</p>
<p>Матрица ошибок представляет собой инструмент,  использующий кросс-табуляцию (<a href="http://en.wikipedia.org/wiki/Cross-tabulation">http://en.wikipedia.org/wiki/Cross-tabulation</a>) для показа  того, как соотносятся значения совпадающих  классов, полученные из различных  источников. В качестве источников могут  выступать, например, проверяемый растр  (тематическая классификация) и опорный  более точный источник данных (растр или  набор полевых данных в виде точек). При  интерпретации результатов обычно  полагается, что проверяемый результат  потенциально является неточным, а  проверочный растр хорошо отражает  реальную ситуацию. В противном случае,  если проверочный растр также несовершенен,  нельзя говорить об «ошибке», а следует  говорить о «разнице» между двумя наборами  данных. Для построения матрицы могут  использоваться все ячейки растра  (пиксели) или выборка ячеек, расположенных  случайно, стратифицировано случайно  или согласно какому-либо другому  распределению.
<p>По одной из осей  матрицы записываются названия классов  легенды классификации проверяемого  набора данных, по второй — классы легенды  данных, используемых для проверки.
<p ALIGN="center"><img src="/images/error-matrix-01.gif" alt="" width="427" height="200">
<p>Серым  отмечена главная диагональ матрицы,  показывающая случаи, где расчетные  классы и реальные данные совпадают  (правильная классификация). Сумма  значений диагональных элементов  показывает общее количество правильно  классифицированных пикселей, а отношение  этого количества к общему количеству  пикселей в матрице N называется общей точностью  классификации и обычно выражается в  процентах:
<p ALIGN="center"><img src="/images/error-matrix-02.gif" alt="" width="269" height="31">
<p>Для  определения точности определенного  расчетного класса, необходимо разделить  количество правильно классифицированных  пикселей этого класса на общее количество  пикселей в этом классе  согласно  проверочным данным. Этот показатель  также называют «точностью производителя» (producer's accuracy), так  как он показывает, насколько хорошо  результат классификации для этого  класса совпадает с проверочными данными.  Для класса A:
<p ALIGN="center"><img src="/images/error-matrix-03.gif" alt="" width="172" height="31">
<p>Похожий  показатель может быть вычислен для  реального класса, если разделить  количество правильно классифицированных  пикселей класса на общее количество  пикселей в этом классе согласно  проверяемым данным. Этот показатель  называют «точностью пользователя» (user's accuracy), так  как он показывает пользователю  классификации насколько вероятно, что  данный класс совпадает с результатами  классификации. Для класса A:
<p ALIGN="center"><img src="/images/error-matrix-04.gif" alt="" width="142" height="30">
<p>Вне-диагональные  элементы показывает случаи несовпадения  между расчетными и реальными классами  (ошибки классификации).
<p><strong>Пример 1 Маска пожаров</strong>
<p>Приведем пример реальной ситуации, при желании вы можете повторить все расчеты и вычисления. Допустим,  у нас есть классификации, показывающие  какая территория сгорела, а какая нет.  Одна из этих классификаций сделана на  базе данных AVHRR,  а другая - MODIS.  Например, иллюстрация показывает  результат наложения двух классификаций,  где:
<p>
0 – оба источника определили территорию  	как не сгоревшую;<br> 
  1 – AVHRR определил территорию  	как сгоревшую, MODIS – как  	не сгоревшую;<br> 
  2 - MODIS определил территорию  	как сгоревшую, AVHRR – как  	не сгоревшую;<br>
  3 - оба источника определили  территорию как сгоревшую.
<p ALIGN="center"><img src="/images/error-matrix-05.gif" alt="" width="600" height="413">
<p>В этом случае,  если мы обозначим сгоревшую территорию  как «ДА», а не сгоревшую как «НЕТ», наша  матрица ошибок будет выглядеть следующим  образом:

<p ALIGN="center"><img src="/images/error-matrix-06.gif" alt="" width="254" height="83">
<p>
Рассчитаем общую ошибку и ошибки для  разных классов.
<p ALIGN="center"><img src="/images/error-matrix-07.gif" alt="" width="204" height="28">
<p>Общая точность 83%,  из рисунка очевидно, что решающую роль  в такой высокой точности играет масса  территорий, классифицированных как  несгоревшие обоими источниками.
<p ALIGN="center"><img src="/images/error-matrix-08.gif" alt="" width="218" height="28">
<p>Точность производителя  (producer’s accuracy) для класса сгоревших  территорий – 88%. Высокая  точность производителя означает, что  в проверяемой классификации мало ошибок  омиссии (ommission errors),  т.е.  мало сгоревших  пикселей было пропущено. Другими словами,  небольшое количество пикселей, которые были на самом  деле (согласно проверочному набору)  сгоревшими, были ошибочно классифицированы как  несгоревшие.
<p align="center"><img src="/images/error-matrix-09.gif" alt="" width="188" height="28"></p>
<p>Точность пользователя (user’s accuracy) для класса сгоревших  территорий – 54%. Низкая  точность пользователя означает, что в  проверяемой классификации много ошибок  комиссии (commission errors),  т.е. много пикселей, которые не сгорели,  но были классифицированы как сгоревшие.
<p ALIGN="center"><img src="/images/error-matrix-10.gif" alt="">

<p>Разберем интерпретацию  точностей для класса сгоревших территорий,  как целевого класса в данном примере.  Как можно видеть, для этого класса  точность производителя значительно  лучше точности пользователя, что в  переводе на человеческий язык означает,  что при производстве данного набора  данных предпочтение было отдано тому,  что «лучше, чтобы все территории которые  на самом деле сгорели, были классифицированы  как сгоревшие», а не «лучше, чтобы  сгоревших территорий было меньше, но  все они были точно сгоревшими».<p>Как видно из примера, ошибки  комиссии и омиссии для одного класса  часто являются противоположными, высокое  значение одной из них часто связано с  низким значением другой. Интерпретация  качества классификации зависит от  ставящихся перед ней задач, обычной  стратегией является нахождение  максимального значения обоих типов  ошибок.
<p><strong>Пример 2</strong>
<p>Более сложный пример, с большим  количеством классов (<a href="http://www.ccrs.nrcan.gc.ca/glossary/index_e.php?id=3124" target="_blank" class="external">источник</a>): 
<p>Количество классов q =  5.
<p ALIGN="center"><img src="/images/error-matrix-11.gif" alt="" width="394" height="124">
<p>Рассчитаем общую точность, точность производителя и пользователя:
<p ALIGN="center"><img src="/images/error-matrix-12.gif" alt="" width="317" height="521">
<p><a href="/other/error_matrix.zip">Расчеты всех показателей точности</a> для приведенных выше данных в формате MS Excel XLS. 

<p class="discuss"><span><!--#include virtual="/scripts/forum-comments-num.php?i=4601"--></span></p>

<!-- ссылки -->
<div class="links">
	<h2>Ссылки по теме</h2>
		<ul>
			<li><a href="/qa/geoproc-python-ag.html" target="_blank" class="external">Обработка данных на языке Python в ArcGIS на примере валидации</a> </li>
			</ul>
</div>
<!--Contents end-->
<!--#include virtual="/scripts/date.php" -->
<p class="status"><span>Дата создания: 06.01.2010
<br>Автор(ы): <a href="/forum/memberlist.php?mode=viewprofile&u=6901" target="_blank">Денис Рыков</a></span></p>
<!--#include virtual="/inc/footer2.php" -->
