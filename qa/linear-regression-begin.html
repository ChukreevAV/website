<!--#include virtual="/inc/header02.txt" -->
<title>GIS-Lab: Интерпретация значения линейной регрессии в R</title>
<!--#include virtual="/inc/header2.txt" -->
<div class="cont">
<div class="col1">

<ul class="path">
   <li class="first"><a href="/">Главная</a></li>
   <li><a href="/qa.html">Вопросы и ответы</a></li>
</ul>
<!--Contents start-->
<h1>Интерпретация отчета о простой линейной регрессии в R</h1>
<p class="ann">Как интерпретировать отчеты о линейных регрессиях в R </p>
            <p>Эта статья описывает информацию, выдаваемую в отчете о линейной регрессии в R и интерпретацию составляющих частей этой информации.</p>
            <p>Отчет о линейной регрессии в R это конструктор, собираемый из различных операций и вычислений. Некоторые из этих вычислений мы воспроизведем в этой статье. </p>
            <p><strong>Оглавление</strong></p>
            <ol>
                  <li><a href="#gen">Общая информация </a></li>
                  <li><a href="#lm">Получение модели и отчета</a> </li>
                  <li><a href="#interpret">Составляющие отчета и их интерпретация</a></li>
            </ol>


<!-- Первый раздел -->
            <h2><strong><a name="01" id="01"></a>1. Общая информация </strong></h2>
            <p>Цель регрессии: оценить насколько сильна связь между зависимой и независимой переменной. </p>
            <p>Основное уравнение линейной регрессии: </p>
            <p>y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &epsilon;</p>
            <p>y - зависимая переменная, в нашем случае logfire</p>
            <p>x - независимая переменная, в нашем случае ndvi5</p>
            <p>&beta;<sub>0</sub>,&beta;<sub>1</sub> - коэффициенты регрессии, intercept и slope </p>
            <p>&epsilon; - ошибка, распределенная независимо и нормально. </p>
            <h2><a name="01" id="01"></a> 2. Получение модели и отчета</h2>
            <p>Данные используемые в этой статье:</p>
            <pre>logfire = c(3.367296,2.564949,3.465736,1.386294,3.091042,3.988984,0.000000,5.141664,0.000000,3.044522,3.828641,6.656727,7.494986,4.762174,8.148156,5.556828,7.957527,4.897840,6.981006,7.521859,8.120886,7.920083,5.971262)
ndvi5 = c(3087,2860,3336,3441,3632,4491,4027,4258,3530,3471,3294,3985,3886,3198,4469,3586,4624,3443,4099,4350,4391,3552,4208)</pre>
            <p>logfire - сгоревшая территория, зависимая переменная</p>
            <p>ndvi5 - индекс NDVI в один из месяцев, независимая переменная  </p>
            <p>Построим диаграмму распределения данных:</p>
            <pre>plot(logfire~ndvi5)
abline(lm(logfire~ndvi5))</pre>
            <p>&nbsp;</p>
            <p align="center"><img src="/images/linear-regression-report-01.gif" width="350" height="321"></p>
            <p>Линейная регрессия переменных logfire и ndvi5:</p>
            <pre>model = lm(logfire~ndvi5)
summary(model)

Call:
lm(formula = logfire ~ ndvi5)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5000 -0.9668  0.4127  1.2783  3.7064 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -5.4046028  3.5193737  -1.536  0.13955   
ndvi5        0.0027079  0.0009203   2.942  0.00778 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 2.18 on 21 degrees of freedom
Multiple R-squared: 0.2919,     Adjusted R-squared: 0.2582 
F-statistic: 8.657 on 1 and 21 DF,  p-value: 0.00778.</pre>

            <h2>
              <!-- Второй раздел -->
              <a name="01" id="01"></a> 3. Составляющие отчета и их интерпретация</h2>
            <p>Для подзаголовков используются английские названия составляющих отчета.</p>
            <p><strong>Call</strong></p>
            <pre>
Call:
lm(formula = logfire ~ ndvi5) </pre>
            <p>Описывает, каким образом была получена модель, какие использовались переменные и как выглядела формула регрессии. </p>
            <p><strong>Residuals</strong></p>
            <pre>
Residuals:
    Min      1Q  Median      3Q     Max 
-5.5000 -0.9668  0.4127  1.2783  3.7064</pre>
            <p><a href="http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics" target="_blank" class="external">Остатки (ошибки)</a> - суммарная статистика по отклонениям реальных значений от прямой регрессии по оси Y. Количество остатков равно количеству данных, т.е. значений в таблице. Получить остатки всех точек (у нас их 23), а не суммарные значения, можно так:</p>
            <pre>prediction = predict(model)
residuals = logfire - prediction</pre>
            <p>или, эквивалентно:</p>
            <pre>residuals = resid(model)</pre>
            <p>Графически остатки точек можно визуализировать. На иллюстрации ниже: прозрачные кружки - наши данные (по X - ndvi5, по Y - logfire), линия - линейная регрессия, непрозрачные кружки - значения согласно описывающей модели. Сегменты соединяющие непрозрачные и прозрачные кружки - остатки/ошибки каждой точки данных.</p>
            <p align="center"><img src="/images/linear-regression-report-02.gif" width="750" height="235"></p>
            <p>Код для графика:</p>
            <pre>plot(ndvi5,logfire)
abline(lm(model),lwd=2)
points(ndvi5,predict(model),pch=19)
segments(ndvi5[seq(23)],fitted(model),ndvi5[seq(23)],logfire)</pre>
            
            <p>Далее можно использовать стандартные функции, для получения тех же значений, что и в отчете регрессии:</p>
            <pre>min(residuals)

median(residuals)
max(residuals)</pre>
            <p>Соответственно: -5.499958, 0.4127282, 3.706359  </p>
            <p>Интерпретация: Таблица остатков дает более подробное, чем R<sup>2</sup>, представление о качестве соответствия реальных данных моделируемым линейной регрессией. В идеале, значение минимума и максимума должны быть минимизированы, а медиана близка к нулю. Наличие больших абсолютных значений min/max остатков численно похожих на сами данных может индицировать выбросы. Например, в нашем примере max(logfire) = 8.148156, а min(residuals) = -5.499958, что говорит о том, что по меньшей мере одна из точек отстоит от прямой регрессии более, чем на 5 единиц. </p>
            <p><strong>Коэффициенты (Coefficients)</strong></p>
            <pre>Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept) -5.4046028  3.5193737  -1.536  0.13955   
ndvi5        0.0027079  0.0009203   2.942  0.00778 **</pre>
            <p>Таблица коэффициентов показывают основные параметры коэффициентов переменных участвующих в модели, количество + 1 независимых переменных определяет количество строк в таблице. В нашем случае, у нас в модели одна независимая переменная, ndvi5 + intercept, поэтому все параметры (Estimate, Std. Error, t value, Pr(>|t|)) показываются для коэффициентов каждого из этих двух участников: &beta;<sub>0</sub> для intercept и  &beta;<sub>1</sub> для ndvi5. Если бы переменных было больше, строк в отчете тоже было бы больше. В случае линейной регресии, удобнее на месте Intercept и ndvi5 представлять &beta;<sub>0</sub> и  &beta;<sub>1</sub>, поскольку в отчете речь идет именно о параметрах коэффициентов переменных, а не самих переменных. </p>
            <p>Далее по тексту, когда ведется речь о ndvi5, мы используем термин Slope.</p>
            <p><strong> Intercept, Estimate </strong></p>
            <p>Значение коэффициента регрессии &beta;<sub>0</sub></p>
            <p>Показывает значение logfire когда ndvi5 = 0, т.е. пересечение линии регрессии с осью Y. Чаще всего бесполезная величина.</p>
            <p><strong>Slope, Estimate </strong></p>
            <p>Значение коэффициента регрессии &beta;<sub>1</sub></p>
            <p>Интерпретация: с каждым увеличением ndvi5 на единицу, logfire в среднем увеличивается на 0.0027079.</p>
            <p><strong>Slope, Std. Error </strong></p>
            <p>Стандартная ошибка коэффициента регрессии &beta;<sub>1</sub>, расчитывается как отношение дисперсии ошибки к сумме квадратов отклонений от среднего X. </p>
            <pre>sx2 = sqrt(sum((ndvi5 - mean(ndvi5))^2))
se = sqrt(sum(residuals(model)^2)/(n-2))
slopestderr = se/sx2
</pre>
            <p>Что составляет 0.0009203087 и соответствует стандартному отклонению коэффициента регрессии &beta;<sub>1</sub>.</p>
            <p>Интерпретация: В идеале, это значение должно быть намного меньше значения коэффициента регрессии, что будет говорить о более устойчивом характере линии регрессии.</p>
            <p><strong> Intercept/Slope, t value</strong></p>
            <p><a href="http://en.wikipedia.org/wiki/T-statistics" target="_blank" class="external">t-статистика</a> - отношение значения коэффициента регрессии к его стандартному отклонению (ошибке), т.е. Estimate/Std. Error. </p>
            <p><strong> Intercept/Slope, Pr(>|t|)</strong></p>
            <p><a href="http://en.wikipedia.org/wiki/P-value" target="_blank" class="external">p-value</a> - вероятность получить результат такой же как наблюдаемый (cм. диаграмму распределения), предполагая, что нуль-гипотеза верна, то есть угол наклона линии регрессии равен 0 или в статистической нотации  P(|t &gt; ti|, |bi = 0). Чем меньше p-value, тем менее вероятен полученный результат. Малое значение p-value ведет к опровержению нуль-гипотезы. </p>
            <p>После определения значения t (см. выше), значение p для соответствующих t и степеней свободы смотрится в соответствующей таблице или вычисляется следующим образом (используется модуль t):</p>
            <pre>(1 - pt(1.536,21))*2
(1 - pt(2.942,21))*2</pre>
            <p>Интерпретация: Так же как и t value, p-value в отчете регрессии показывает значение для индивидуального теста нуль-гипотезы: &quot;Intercept = 0&quot; или &quot;Slope = 0&quot;. Другими словами, можем ли мы опровергнуть гипотезу что intercept/slope = 0. Если p-value меньше 0.05, нуль-гипотеза может быть отвергнута, т.е. значение не равно 0 и мы не можем от него избавиться. В нашем примере это так, т.е. переменная важна. Если p-value &gt; 0.05, нуль-гипотеза   не может быть отвергнута, однако это не означает, что intercept действительно = 0, а только то, что гипотеза не может быть отвергнута. </p>
            <p><strong>Residual standard error</strong></p>
            <pre>
Residual standard error: 2.18 on 21 degrees of freedom
</pre>
            <p>Стандартная ошибка остатков является оценкой стандартного отклонения ошибки &epsilon; (см. уравнение регрессии).</p>
            <p>Сумма квадратов остатков (SSError)</p>
            <pre>SSError = sum(residuals(model)^2)
SSError = deviance(model)
</pre>
            <p>Средняя ошибка (MSError) - дисперсия ошибки (в нашем случае 4.753048). </p>
            <pre>MSError = SSError/(n-2)
</pre>
            <p>Наконец, для получения стандартного отклонения ошибки из ее дисперсии, нужно взять из нее квадратный корень:</p>
            <pre>RSS = sqrt(MSError) </pre>
            <p>В нашем случае 2.180149, что соответствует значению из таблицы. </p>
            <p>В полной форме выводы приведенные выше будут выглядеть таким образом: </p>
            <pre>sqrt(sum(residuals(model)^2)/(n-2))
sqrt(deviance(model)/df.residual(model))</pre>
            <p>Интерпретация: Стандартная ошибка остатков является еще одним интегральным, то есть для регрессии целиком, показателем качества соответствия регрессии прямой линии. Зная RSS, мы можем моделировать ошибку, например для симуляции данных, чтобы они имели такое же ее распределение:</p>
            <pre>err = rnorm(n,mean=0,sd=RSS)</pre>
            <p> </p>
            <p><strong>Multiple R-squared</strong></p>
            <pre>
Multiple R-squared: 0.2919,     Adjusted R-squared: 0.2582 
</pre>
            <p><a href="http://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank" class="external">Коэффициент детерминации</a> - квадрат корреляции (R) R<sup>2</sup>. Статистический показатель того, насколько хорошо линия регрессии описывает реальные данные. R<sup>2</sup> = 1 показывает, что совпадение идеальное. В случае если используется метод наименьших квадратов,  R<sup>2</sup> увеличивается при увеличении количества переменных в модели, что может создать ложное впечатление о увеличении качестве модели, для более объективного показателя в этом случае используется Adjusted R-squared. </p>
            <p>Другой способ получить коэффициент - из отчета о модели:</p>
            <pre>summary(lm1)$r.squared
summary(lm1)$adj.r.squared</pre>
            <p>Корреляция (R) переменных logfire и ndvi5:</p>
            <pre>cor(logfire,ndvi5)</pre>
            <p>Результат: 0.5402897</p>
            <p> R<sup>2</sup> = 0.5402897^2 = 0.291913, можно видеть, что результирующее значение соответствует значению Multiple R-squared в отчете о линейной корреляции. Справедливо только в случае, если независимая переменная одна. </p>
            <p>Интерпретация: 29% переменной logfire объясняется переменной ndvi5.</p>
            <p><strong>Adjusted R-squared</strong></p>
            <pre>
Multiple R-squared: 0.2919,     Adjusted R-squared: 0.2582</pre>
            <p>Служит для случаев, когда количество независимых переменных больше чем 1, уменьшает R<sup>2</sup> при увеличении количества переменных. </p>
            <p>Adjusted R<sup>2</sup> = R<sup>2</sup> -  (1-R<sup>2</sup>)*(k-1)/(n-k), где k = количество переменных, n = количество данных</p>
            <p>В нашем случае:</p>
            <p>Adjusted R<sup>2</sup> =   0.2919 - (1-0.2919)*(2-1)/(23-2) =  0.2581810, можно видеть, что это соответствует Adjusted R-squared из таблицы.</p>
            <p>В случае одной переменной Adjusted R<sup>2</sup> использовать не обязательно. </p>
            <p><strong>F-statistic</strong></p>
            <pre>
F-statistic: 8.657 on 1 and 21 DF,  p-value: 0.00778.</pre>
            <p><a href="http://en.wikipedia.org/wiki/F-test" target="_blank" class="external">F-статистика</a> - статистика имеющая F-распределение, если нуль-гипотеза верна. Используется в F-тесте для сравнения нескольких моделей. Показатель теста эквивалетен T-тесту, с нуль-гипотезой гласящей, что slope = 0, H<sub>0</sub>: &beta;<sub>1</sub> = 0. </p>
            <p>Для расчета F-статистики сначала вычисляется сумма квадратов регрессии (SSRegression): </p>
            <pre>SSRegression = sum((logfire - residuals - mean(logfire))^2)</pre>
            <p>В нашем случае SSRegression = 41.1489. Так как количество степеней свободы для SSRegression = 1, то среднее квадратов регрессии = сумме квадратов регрессии:</p>
            <pre>MSRegression = SSRegression</pre>
            <p>F-статистика является отношением среднего квадратов регрессии и дисперсии ошибки (см. RSS): </p>
            <pre>F = MSRegression/MSError</pre>
            <p>41.1489/4.753048 = 8.657371</p>
            <p>Интерпретация: Результирующая F-статистика сравнивается с критическим значением F-статистики, которая берется из таблиц, (F<sub>1,21</sub>= 4.32 ) например для уровня достоверности 0.05. Если результирующее значение больше, чем критическое, нуль-гипотеза отвергается. Другой способ вычисления p-value, без таблиц, см. ниже. </p>
            <p><strong>DF</strong></p>
            <pre>
F-statistic: 8.657 on 1 and 21 DF,  p-value: 0.00778.</pre>
            <p>Степени свободы. Так как определяются два параметры (intercept и slope), используется N-2 степеней свободы. В нашем примере N=23, DF = 21. </p>
            <p><strong>p-value</strong></p>
            <pre>
F-statistic: 8.657 on 1 and 21 DF,  p-value: 0.00778.</pre>
            <p>Другой способ получить p-value для F-теста является следующая команда в R: </p>
            <pre>1 - pf(8.657,1,21)</pre>
            <p>p-value = 0.007781192</p>
            <p>Интепретация: такая же, как и у значения Pr(&gt;|t|) для slope, если p-value меньше 0.05, отвергаем H<sub>0</sub>, slope не равен 0.</p>
            <p><span>
              <!--#include virtual="/scripts/forum-comments-num.php?i=852"-->
            </span>		</p>
            <div class="links">
				<h2>Ссылки по теме</h2>
					<ul>
						<li><a href="modislandprod.html">Ссылка на дополнительный источник информации внутренняя</a></li>
						<li><a href="http://lpdaac.usgs.gov/landdaac/tools/modis/about.asp" target="_blank" class="external">Ссылка на дополнительный источник информации внешняя</a></li>
					</ul>
			</div>
<!--Contents end-->
<!--#include virtual="/scripts/date.php" -->
<!--#include virtual="/inc/footer2.php" -->
